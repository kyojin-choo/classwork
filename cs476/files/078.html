<img alt="CMU Robotics Institute" align=middle src="ri.gif">
<TITLE>Sonar Mapping for Underwater</TITLE>
<H1>Sonar Mapping for Underwater</H1>
<TITLE>  Vehicles</TITLE>
<H1>  Vehicles</H1>
<ADDRESS>Investigators: Andrew E. Johnson and Martial Hebert</ADDRESS>
<HR><H2>Table of Contents</H2>

<HR><!-- This file was created with the fm2html filter.The filter is copyright Norwegian Telecom Research and was programmed by Jon Stephenson von Tetzchner.  -->Generating representations of the underwater environment is a critical component of any autonomous system designed to navigate underwater. This project at the <B>Vision and Autonomous Systems Center</B> addresses the task of building elevation maps of the seafloor for an Autonomous Underwater Vehicle (AUV) using sonar data. Sonar is the preferred sensing modality for AUVs because it is less susceptible to attenuation and refraction by the water column than common terrestrial perception sensors like cameras and laser range finders. Sonar systems designed to directly generate 3D maps of their environment are generally complex or have low resolution, while systems that generate backscatter images of their environment are less complicated and more common. Hence, techniques that generate 3D elevation maps from 2D sonar backscatter images are necessary for terrain modeling and navigation underwater.<p>
We use backscatter data collected by a side-scan sonar system at Woods Hole Oceanographic Institution. This type of sonar returns the backscatter from the observed surface as a function of range from the sensor for each ping of the sonar. If the sensor is towed in straight line then consecutive pings can be placed adjacent to each other to create a backscatter image of the seafloor. We have developed two techniques for the generation of elevation maps of the seafloor from side-scan sonar backscatter images. These techniques employ a scattering model of the seafloor to establish a correspondence between the backscatter at a point and the surface normal at that point. The first technique uses a constraint between the surface normal and the position of the sensor to generate a partial differential equation which, when solved, generates the elevation map of the surface. The second technique uses an iterative relaxation method to generate the surface by minimizing the difference between the intensity data and the calculated surface intensity. This technique is similar to shape from shading methods used in computer vision. In both techniques sparse bathymetric data is used to generate an initial guess for the shape of the seafloor and an initial guess for the scattering model parameters. <p>
These techniques are designed to support different scattering models, so they can be applied to different underwater environments. This is in contrast with other approaches that are generally less flexible with respect to the scattering model used. In addition to the elevation map of the seafloor, the parameters of the scattering model (like albedo and surface roughness) at every point in the image are generated. These parameters describe material properties of the seafloor, so maps of scattering model parameters can be used to segment the seafloor according to material type.<p>
If the sensor is not towed in a straight line, distortions will occur in the backscatter image that degrade the reconstruction of the elevation map. To remove the effects of these distortions we are developing techniques for incorporating knowledge about the movement of the sensor platform into the surface reconstruction process. First the surface is reconstructed locally using the assumption that locally the sensor moves in a straight line. Then these local reconstructions are transformed to a global coordinate system using the known position of the sensor and the reconstruction is done globally on all of the data. To carry out this task, we employ a method for merging sonar data taken from different sensor positions which can also be used in map merging.<p>
<A HREF="sonar.figure.id.1.gif"><img src="sonar.figure.id.1.gif"></A><p>
<A NAME="ENDFILE"><PRE> </PRE></A>
